{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Section 3: Projections, Subspaces, Orthogonality, and QR decomposition\n",
    "\n",
    "In lecture you have been discussing subspaces and the notion of orthogonality. Generating orthogonal subspaces or an orthonormal basis for matrices can be a powerful numerical tool.\n",
    "\n",
    "In this section we will explore this idea of orthogonality and how to use it to describe matrices and solve least squares."
   ],
   "metadata": {
    "id": "1s7Anw7c34pD"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using QR for least squares\n",
    "\n",
    "We can use least squares and QR to attempt to classify handwritten digits from the MNIST dataset. This is essentially a single layer perceptron with no activation function. We will use tensorflow to load the data since it has a nice loader to numpy."
   ],
   "metadata": {
    "id": "U19FClQKBRgJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ],
   "metadata": {
    "id": "0Ifc767FYcc-"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see here that each training example is a 28x28 image. We want each example as a single vector so let's flatten that to shape to make a large data matrix."
   ],
   "metadata": {
    "id": "xHIAgd8HaHj9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x_train = x_train.reshape((60000, 784))\n",
    "bias = np.ones((60000, 1))\n",
    "x_train = np.concatenate((bias, x_train), axis=1)"
   ],
   "metadata": {
    "id": "_k2IRCD_Zr0k"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's call the data matrix $A$ and the labels $b$. There likely isn't a solution to the system $Ax = b$ since the matrix $A$ has many less columns than rows. This means we want to solve $\\min_{x}||Ax - b||_{l_2}$, which is the least squares problem. Let's think about how we can do this using $QR$. \n",
    "\n",
    "Fact:\n",
    "1. Because $Q$ is orthonormal, it doesn't change the norm of any vector.\n",
    "\n",
    "Proof:\n",
    "$$||Qy||^2_2 = (Qy)^T (Qy) = y^TQ^TQy = y^ty = ||y||_2^2$$\n",
    "\n",
    "2. So that means we can transform our minimization problem to:\n",
    "\\begin{align}\n",
    "&=\\min_x ||Ax - b||_2\\\\\n",
    "&=\\min_x ||Q^T(Ax - b)||_2\\\\\n",
    "&=\\min_x ||Q^T(QRx - b)||_2\\\\\n",
    "&=\\min_x ||Rx - Q^T b||_2\n",
    "\\end{align}\n",
    "\n",
    "But since $R$ is an upper triangular matrix, we know there is a solution to:\n",
    "$$Rx = Q^T b$$\n",
    "or\n",
    "$$x = R^{-1} Q^T b$$\n",
    "which would make the result of the minimization $0$."
   ],
   "metadata": {
    "id": "Z0aayBooas_-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Q, R = scipy.linalg.qr(x_train, mode='economic')\n",
    "np.linalg.matrix_rank(R)"
   ],
   "metadata": {
    "id": "i8tHJGQIgNNp"
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "713"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice the above technique only works if $R$ is invertible, or full rank. $R$ will only be full rank if the original data, $A$, has linearly dependent columns. Often times there are linear dependencies in the dataset, meaning we have to take a slightly different approach to the $QR$ for least squares.\n",
    "\n",
    "This process is known as *rank deficient least squares* and requires a modified $QR$ which permutes the $A$ matrix so that the diagonal or $R$ is not increasing. If you're interested, [here is a formal description of this algorithm using householder transformations](https://www.math.usm.edu/lambers/mat610/sum10/lecture11.pdf).\n",
    "\n",
    "For our case, `scipy` offers a `pivoting` argument flag that does this for us."
   ],
   "metadata": {
    "id": "p8XVVNsIavGQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "Q, R_p, p = scipy.linalg.qr(x_train, mode='economic', pivoting=True)"
   ],
   "metadata": {
    "id": "AKBn4nB-cc8P"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This version gives us a $Q$ like before but the $R_p$ is provided in the form of:\n",
    "$$R_p = \\begin{bmatrix}\n",
    "R & S\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix} $$\n",
    "Where the $R$ now is a true upper triangular. The provided $p$ is the pivots required to $A$ to satisfy:\n",
    "$$A \\Pi = Q R_p$$\n",
    "With $\\Pi$ being the permutation matrix created from $p$. We can create that permutation matrix with `np.eye(size)[:,p]`.\n",
    "\n",
    "We checked previously that the rank of our R matrix is 713 but let's check again with the pivoted R by looking for the first 0 in the diagonal."
   ],
   "metadata": {
    "id": "0O6I9K9ohpRw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "rank = np.argmax(np.absolute(np.diag(R_p)) < 1e-6)\n",
    "rank"
   ],
   "metadata": {
    "id": "dUmNC7etgBpO"
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "713"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, one way to solve for $x$ is to slice off the part of $R$ that is all zeros and cut the bottom portion off of $Q^T b$ so that we can solve the triangular system. The resulting $x$ needs to be permuted using the pivots to get an actual solution to the least squares.\n",
    "\n",
    "\\begin{align}\n",
    "||b - Ax||_2^2 &= \\left\\|b - Q\n",
    "\\begin{bmatrix}\n",
    "R & S\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\Pi^T x \\right\\|_2^2\\\\\n",
    "&= \\left \\|Q^T b - \n",
    "\\begin{bmatrix}\n",
    "R & S\\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "u\\\\\n",
    "v\n",
    "\\end{bmatrix}\n",
    "\\right\\|_2^2\\\\\n",
    "&=\\left\\|\n",
    "\\begin{bmatrix}\n",
    "c\\\\\n",
    "d\n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "Ru & Sv\\\\\n",
    "0\n",
    "\\end{bmatrix} \\right\\|_2^2\\\\\n",
    "&= \\|c - Ru - Sv||_2^2 + \\|d\\|_2^2\\\\\n",
    "\\text{where} \\quad\n",
    "Q^Tb &=\n",
    "\\begin{bmatrix}\n",
    "c\\\\\n",
    "d\n",
    "\\end{bmatrix}, \\quad \\Pi^Tx =\n",
    "\\begin{bmatrix}\n",
    "u\\\\\n",
    "v\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In the implementation below, I choose the simplest solution with $v=0$."
   ],
   "metadata": {
    "id": "64IaOosYkbVu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "R = R_p[:rank, :rank]\n",
    "c = Q.T[:rank,:] @ y_train\n",
    "u = scipy.linalg.solve_triangular(R, c, lower=False)\n",
    "v = np.zeros(785 - rank)\n",
    "uv = np.concatenate((u,v))\n",
    "x = np.eye(785)[:,p] @ uv\n",
    "pred = x_train @ x\n",
    "print(x.shape)\n",
    "print(x_train.shape)\n",
    "print(pred.shape)\n",
    "print(pred[:10])\n",
    "print(y_train[:10])"
   ],
   "metadata": {
    "id": "hJ1Et_-Jh4Fi"
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785,)\n",
      "(60000, 785)\n",
      "(60000,)\n",
      "[4.19480069 1.20585109 3.19619527 2.3125187  7.70189204 4.07685415\n",
      " 1.62278595 3.92588536 1.84626138 4.82906635]\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We see that this is *sort of* working...\n",
    "\n",
    "The prediction numbers are approximately the same as the labels but with categorical labels like *digit*, getting a real number that is around the correct value isn't going to make a very good classifier.\n",
    "\n",
    "We can really improve this model by making 10 different binary classifiers by changing the right hand side, $y_{train}$, to be a $1$ or a $0$ depending if it is the digit we are trying to classify."
   ],
   "metadata": {
    "id": "Gw2jJfVTvQDg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3\n",
    "\n",
    "Here we will complete the first step to improve the least squares classifier by implementing it as a binary classifier.\n",
    "\n",
    "Modify `y_train` so that it *one hot encodes* the dataset. One hot encoding is where you take a vector of categorical labels and translate it to a vector for each category. Each category vector should have a $1$ if the observation is that category and a $0$ otherwise.\n",
    "\n",
    "### One hot encoding example:\n",
    "$$\n",
    "\\begin{bmatrix} 3\\\\ 2\\\\ 3\\\\ 1\\\\ 0 \\end{bmatrix} \\to\n",
    "\\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1 \\end{bmatrix}, \n",
    "\\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 0 \\end{bmatrix}, \n",
    "\\begin{bmatrix} 0\\\\ 1\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix}, \n",
    "\\begin{bmatrix} 1\\\\ 0\\\\ 1\\\\ 0\\\\ 0 \\end{bmatrix}\n",
    "\\quad \\text{or in matrix form:} \\quad\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 \\\\\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These new one hot encoded vectors will become the new $b$ in the least squares formulation (formerly `y_train`)."
   ],
   "metadata": {
    "id": "nrXAjqSBvWE_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(y_train)\n",
    "#make a np array of 10 vectors that are len(y_train) long\n",
    "hot_encode_matrix = np.zeros((len(y_train), 10))\n",
    "print(hot_encode_matrix.shape)\n",
    "print(hot_encode_matrix[3000][9])\n",
    "#assumes that the elements are all between 0 through 9\n",
    "for i in range(len(y_train)):\n",
    "    hot_encode_matrix[i][y_train[i]] = 1\n",
    "\n",
    "print(hot_encode_matrix)"
   ],
   "metadata": {
    "id": "Akq-mvDUy3Fn"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ... 5 6 8]\n",
      "(60000, 10)\n",
      "0.0\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Homework 3\n",
    "\n",
    "After creating the one hot encoding of the labels, use the $Q$ and $R$ from the code above to solve the least squares problem for $x$ for each of these 10 one hot encoded column vectors.\n",
    "\n",
    "Concat each $x$ into a matrix, $X = \\begin{bmatrix} x_1, & x_2, & \\dots, & x_{10} \\end{bmatrix}$\n",
    "\n",
    "And get the resulting prediction matrix, $Y = A X$.\n",
    "\n",
    "For each row (observation) of this prediction matrix, you will have 10 values that correspond to each label. Whichever value is highest is the prediction. Extract the index of the highest value for each row in this matrix. Compare the predicted labels with the actual labels to calculate your prediction accuracy.\n",
    "\n",
    "### Bonus / Extra Credit\n",
    "\n",
    "Create a confusion matrix of the result and make a short comment about your observations of this matrix."
   ],
   "metadata": {
    "id": "nyq8Gctty7C2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "60000\n",
      "9\n",
      "85.77333333333334  % accurate\n"
     ]
    }
   ],
   "source": [
    "X = np.zeros((60000,10))\n",
    "print(X.shape)\n",
    "for i in range(0, hot_encode_matrix.shape[1]):\n",
    "    c = Q.T[:rank,:] @ hot_encode_matrix[:,i]\n",
    "    u = scipy.linalg.solve_triangular(R, c, lower=False)\n",
    "    v = np.zeros(785 - rank)\n",
    "    uv = np.concatenate((u,v))\n",
    "    x = np.eye(785)[:,p] @ uv\n",
    "    #instead of next line, do an np thing to add the data\n",
    "    X[:,i] = x_train @ x\n",
    "# print(X[:3])\n",
    "max_indices = np.argmax(X, 1)\n",
    "max_values = X[np.arange(len(X)),max_indices]\n",
    "#print(max_indices.shape)\n",
    "#print(len(max_indices))\n",
    "#print(max(max_indices))\n",
    "#print(max_values.shape)\n",
    "# confusion matrix: each pair in above are on the diagonal\n",
    "# start w 10x10 zeros\n",
    "# increment at each pair max_i,ytrain_i\n",
    "confusion_matrix = np.zeros((10,10))\n",
    "correct = 0\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i] == max_indices[i]):\n",
    "        correct += 1\n",
    "    confusion_matrix[y_train[i]][max_indices[i]] += 1\n",
    "\n",
    "# print(confusion_matrix)\n",
    "correctness = correct/60000\n",
    "print(correctness * 100, \" % accurate\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A note about least squares in practice\n",
    "\n",
    "Please don't implement least squares on your own like this in practice, `numpy` has least squares implemented for you."
   ],
   "metadata": {
    "id": "UPV94H8T149m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result = np.linalg.lstsq(x_train, y_train, rcond=1e-6)\n",
    "pred = x_train @ result[0]\n",
    "\n",
    "print(pred[:10])\n",
    "print(y_train[:10])"
   ],
   "metadata": {
    "id": "oXRPs-_hqTQR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.19480069 1.20585109 3.19619527 2.3125187  7.70189204 4.07685415\n",
      " 1.62278595 3.92588536 1.84626138 4.82906635]\n",
      "[5 0 4 1 9 2 1 3 1 4]\n"
     ]
    }
   ]
  }
 ]
}